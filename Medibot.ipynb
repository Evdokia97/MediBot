{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghFR8GYZd7S7"
      },
      "source": [
        "## ***Step 0:*** Import the Necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1LPW7bmd7TH"
      },
      "source": [
        "0.1 Install the necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXa77nInd7TJ",
        "outputId": "63bf67a8-2299-4951-936a-36fca6fb9748"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: sentence-transformers in /home/evi/anaconda3/lib/python3.7/site-packages (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.18.1)\n",
            "Requirement already satisfied, skipping upgrade: nltk in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (3.4.5)\n",
            "Requirement already satisfied, skipping upgrade: transformers<5.0.0,>=4.6.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: huggingface-hub in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied, skipping upgrade: sentencepiece in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.1.91)\n",
            "Requirement already satisfied, skipping upgrade: torchvision in /home/evi/anaconda3/lib/python3.7/site-packages (from sentence-transformers) (0.11.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /home/evi/anaconda3/lib/python3.7/site-packages (from nltk->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml>=5.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.3)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.11.10)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.22.0)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /home/evi/anaconda3/lib/python3.7/site-packages (from scikit-learn->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /home/evi/anaconda3/lib/python3.7/site-packages (from huggingface-hub->sentence-transformers) (4.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pillow!=8.3.0,>=5.3.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from torchvision->sentence-transformers) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /home/evi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=3.0.5,>=2.0.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.25.8)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: nltk in /home/evi/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
            "Requirement already satisfied: six in /home/evi/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
            "Requirement already satisfied: sklearn in /home/evi/anaconda3/lib/python3.7/site-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /home/evi/anaconda3/lib/python3.7/site-packages (from sklearn) (0.22.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/evi/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.18.1)\n",
            "Requirement already satisfied: transformers in /home/evi/anaconda3/lib/python3.7/site-packages (4.15.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: filelock in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (5.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
            "Requirement already satisfied: requests in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: sacremoses in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from transformers) (1.18.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /home/evi/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (2.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from packaging>=20.0->transformers) (2.4.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.25.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: click in /home/evi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /home/evi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\n",
            "Requirement already satisfied: joblib in /home/evi/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/evi/anaconda3/lib/python3.7/site-packages (1.10.1)\n",
            "Requirement already satisfied: typing-extensions in /home/evi/anaconda3/lib/python3.7/site-packages (from torch) (4.0.1)\n",
            "Requirement already satisfied: tensorflow in /home/evi/anaconda3/lib/python3.7/site-packages (2.7.0)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.19.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (12.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (4.0.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.23.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.7.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.43.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (2.3.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from tensorboard~=2.6->tensorflow) (45.2.0.post20200210)\n",
            "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/evi/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/evi/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/evi/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/evi/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /home/evi/anaconda3/lib/python3.7/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /home/evi/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: torchinfo in /home/evi/anaconda3/lib/python3.7/site-packages (1.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install nltk\n",
        "!pip install sklearn\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install tensorflow\n",
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IRew4VHd7TO"
      },
      "source": [
        "0.2 Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbKcgKLjd7TR"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import torch\n",
        "import io\n",
        "import random\n",
        "import string # to process standard python strings\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import json\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3a4Se3od7TT"
      },
      "source": [
        "0.3 Usefull Info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1sOyjQwd7TU"
      },
      "source": [
        "1.https://theaidigest.in/how-to-do-semantic-document-similarity-using-bert/com/geekculture/simple-chatbot-using-bert-and-pytorch-part-1-2735643e0baa\n",
        "2.https://towardsdatascience.com/bert-nlp-how-to-build-a-question-answering-bot-98b1d1594d7b\n",
        "3.https://pretagteam.com/question/how-to-use-bert-for-document-similarity-of-this-site\n",
        "4.https://www.kite.com/python/answers/how-to-read-a-text-file-into-a-list-in-python\n",
        "5.https://reposhub.com/python/deep-learning/MartinoMensio-spacy-sentence-bert.html\n",
        "7.https://pypi.org/project/sentence-transformers/0.3.2/\n",
        "6.https://stackoverflow.com/questions/52074153/cannot-convert-list-to-array-valueerror-only-one-element-tensors-can-be-conver   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYP6bARTd7TW"
      },
      "source": [
        "## ***Step 1:*** Download SQuAD 2.0 dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "628t9lFyd7TX"
      },
      "source": [
        "The ***Stanford Question Answering Dataset*** (SQuAD) consists of questions (100,000)  on a set of Wikipedia articles where the answer to every question is a segment of text, or span, from the corresponding reading passage. In second version of SQuAD unanswerable questions were added (50,000) to look similar to answerable ones.   \n",
        "\n",
        "https://rajpurkar.github.io/SQuAD-explorer/explore/v2.0/dev/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SR6IeaChd7TY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad/train-v2.0.json\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json -O squad/dev-v2.0.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUz1Vmmvd7TZ"
      },
      "source": [
        "## ***Step 2:*** Retrieve and Store the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtayXlnQd7Ta"
      },
      "source": [
        "2.1 Store the texts, queries and answers of SQUAD dataset from the train and validation .json files. Informations are saved into lists.\n",
        "Training data: It has the input data together with correct/expected output; This dataset is usually duly prepared either by humans or by collecting some data in a semi-automated way. But you must have the expected output for every data row here because you need this for supervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92KmqxHTd7Tb"
      },
      "outputs": [],
      "source": [
        "# Give the path for train data\n",
        "path = Path('squad/train-v2.0.json')\n",
        "\n",
        "# Open .json file\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "\n",
        "\n",
        "# Search for each passage, its question and its answer\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        texts.append(context)\n",
        "\n",
        "train_texts = texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v51t1kmOd7Tb"
      },
      "source": [
        "2.2 Open, process and store data file from PubMed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYfiMxhpd7Tc"
      },
      "outputs": [],
      "source": [
        "my_file = open(\"myfile\", \"r\")\n",
        "content = my_file.read()\n",
        "content = content.split('.')\n",
        "my_file.close()\n",
        "\n",
        "#print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jS13K7id7Te"
      },
      "source": [
        "2.3 Join the PubMed and Squad lists\n",
        "We keep a small part of SQUAD data(20) in order to run our example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KrFfdzQd7Tf",
        "outputId": "e80f2079-0310-4cc7-9a64-565d5318916c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8737\n"
          ]
        }
      ],
      "source": [
        "train_texts = train_texts[:220]\n",
        "content=content+train_texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du3YCx0zd7Th"
      },
      "source": [
        "## ***Step 3:*** Tokenize passages and queries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flAqfHqid7Th"
      },
      "source": [
        "3.1 BERT-base pretrained model “roberta-large-nli-stsb-mean-tokens” for the tokenization.\n",
        "Train the data\n",
        "\n",
        "1.roberta-large-nli-stsb-mean-tokens\n",
        "\n",
        "2.bert-large-nli-stsb-mean-tokens\n",
        "\n",
        "3.xlm-r-large-en-ko-nli-ststb\n",
        "\n",
        "4.monologg/biobert_v1.1_pubmed\n",
        "\n",
        "\n",
        "\n",
        "https://openprojectrepo.com/project/MartinoMensio-spacy-sentence-bert-python-deep-learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f9f359bd9a7c4882906480384136b6bf"
          ]
        },
        "id": "xCxJQKR9d7Tj",
        "outputId": "9c9393d7-64eb-4b9b-d638-305f1b21f15e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9f359bd9a7c4882906480384136b6bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
        "#model = SentenceTransformer('bert-large-nli-stsb-mean-tokens')\n",
        "#model = SentenceTransformer('monologg/biobert_v1.1_pubmed')\n",
        "model = SentenceTransformer('xlm-r-large-en-ko-nli-ststb')\n",
        "text_embeddings = model.encode(content, batch_size = 8, show_progress_bar = True)\n",
        "print(text_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK4Hwv8sd7Tk"
      },
      "source": [
        "3.2 Load the fine tuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4327k-Ud7Tl"
      },
      "outputs": [],
      "source": [
        "#model = torch.load(\"/home/evi/Desktop/MedibotModel/robertamodel\",map_location=torch.device('cpu'))\n",
        "#text_embeddings = model.encode(content, batch_size = 8, show_progress_bar = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9LMSY6Td7Tm"
      },
      "source": [
        "3.3 Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_OQZzh2d7Tn"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "torch.save(model,\"/home/evi/Desktop/MedibotModel/xlm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X36Miy3Ld7To"
      },
      "source": [
        "## ***Step 4:*** Create Chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EdkfzAxd7To"
      },
      "source": [
        "4.1 Create some standard answers if the user is greeting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM-vbvKTd7Tp"
      },
      "outputs": [],
      "source": [
        "#some standard in and outs\n",
        "\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"whats up\",\"hey\")\n",
        "GREETING_RESPONSES = [\"hello\", \"hi\", \"whats up\",\"hey\"]\n",
        "def greeting(sentence):\n",
        " \n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyG3WwgYd7Tp"
      },
      "source": [
        "4.2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MPv9ZS9Ud7Tq"
      },
      "outputs": [],
      "source": [
        "def index_sort(list_var):\n",
        "    length = len(list_var)\n",
        "    list_index = list(range(0, length))\n",
        "    \n",
        "    x=list_var\n",
        "    for i in range(length):\n",
        "        for j in range(length):\n",
        "            if x[list_index[i]] > x[list_index[j]]:\n",
        "                #swap\n",
        "                temp = list_index[i]\n",
        "                list_index[i] = list_index[j]\n",
        "                list_index[j] = temp\n",
        "    return list_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFftOEnDd7Tr"
      },
      "source": [
        "4.3 Create a response for the input question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxv4Lpzid7Tr"
      },
      "outputs": [],
      "source": [
        "def response(question):\n",
        "    \n",
        "    quest=[]\n",
        "    quest.append(question)\n",
        "\n",
        "    answer=''\n",
        "    user_embeddings = model.encode(quest, batch_size = 8, show_progress_bar = False)\n",
        "    np.shape(user_embeddings)\n",
        "\n",
        "\n",
        "    usersim = cosine_similarity(user_embeddings,text_embeddings)\n",
        "    usersim_sorted = usersim.argsort()\n",
        "    similarity_scores_list = usersim.flatten()\n",
        "    index = index_sort(similarity_scores_list)\n",
        "    index = index[1:]\n",
        "    response_flag = 0\n",
        "    j = 0\n",
        "    for i in range(len(index)):\n",
        "        if similarity_scores_list[index[i]]>0.3 :\n",
        "            answer = answer+' '+content[index[i]]+'.'\n",
        "            response_flag = 1\n",
        "            j = j+1\n",
        "        if j > 2:\n",
        "            break\n",
        "            \n",
        "            \n",
        "\n",
        "    if response_flag == 0:\n",
        "        answer=\"I think I need to read more about that...\"    \n",
        "        \n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCjGhpnsd7Ts"
      },
      "source": [
        "4.4 Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VagKNpfd7Tu"
      },
      "outputs": [],
      "source": [
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHIUmmeBd7Tv"
      },
      "source": [
        "4.4.2  \n",
        "\n",
        "Model F1 score represents the model score as a function of precision and recall score. F-score is a machine learning model performance metric that gives equal weight to both the Precision and Recall for measuring its performance in terms of accuracy, making it an alternative to Accuracy metrics (it doesn’t require us to know the total number of observations). It’s often used as a single value that provides high-level information about the model’s output quality. This is a useful measure of the model in the scenarios where one tries to optimize either of precision or recall score and as a result, the model performance suffers. \n",
        "\n",
        "\n",
        "Precision\n",
        "Precision identifies the frequency of correct answers, when the prediction is intent A​. It can be thought of as the answer to the question “Out of all predictions of A, how many were correct?”\n",
        "\n",
        "\n",
        "Recall\n",
        "Recall identifies the frequency of detecting A, out of all examples pertaining to A​ in reality. In short, it answers the question “out of all the examples in A, how many were detected?”\n",
        "\n",
        "https://towardsdatascience.com/classification-metrics-how-to-boost-your-bot-performance-through-data-525349352828\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBziIeUkd7Tw"
      },
      "outputs": [],
      "source": [
        "def compute_f1(myans,ans):\n",
        "\n",
        "  myans_tokens=normalize_text(myans).split()\n",
        "  ans_tokens=normalize_text(ans).split()\n",
        "  print('Tokens of my answer are:',myans_tokens)\n",
        "  print('Tokens of Medibot','s answer are:',ans_tokens)\n",
        "\n",
        "  if len(ans_tokens) == 0 or len(myans_tokens) ==0:\n",
        "    int(ans_tokens == myans_tokens)\n",
        "\n",
        "  common_tokens = set(ans_tokens) & set(myans_tokens)\n",
        "  print('The common Tokens are:', len(common_tokens))\n",
        "  print('The lenght of my answer is:',len(set(myans_tokens)))\n",
        "  print('The lenght of Medibot','s answer is:',len(set(ans_tokens)))\n",
        "\n",
        "\n",
        "  if len(common_tokens) == 0:\n",
        "    print('------')\n",
        "  else:\n",
        "    prec= len(common_tokens)/len(set(ans_tokens))\n",
        "    rec= len(common_tokens)/len(set(myans_tokens))\n",
        "    f1 = 2* (rec*prec)/(rec+prec)\n",
        "    print('\\nPrecision:',prec,'\\n')\n",
        "    print('Recall:',rec,'\\n')\n",
        "    print('F1:',f1,'\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9VYcScpd7Tw"
      },
      "source": [
        "4.4.3 Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hzkIomGd7Tx"
      },
      "outputs": [],
      "source": [
        "def evaluation():\n",
        "    flag=True\n",
        "    print(\"\\nMediBot: Hello \"+name+\"! Please enter the question you want to test.\"+\"\\n\\n\"+name+\":\")\n",
        "    while(flag==True):\n",
        "  #  user_choice='no'\n",
        "        user_response = input()\n",
        "        user_response=user_response.lower()\n",
        "        if(user_response!='bye'):\n",
        "                answer=response(user_response)\n",
        "                print(\"\\nPlease enter your prediction for this answer\\n\\n\"+name+\":\")\n",
        "                prediction=input()\n",
        "                print(\"\\nMediBot's answer:\",end=\"\")\n",
        "                print(answer)\n",
        "                #print(\"\\n----------------------------\\n\",end=\"\")\n",
        "                #print(\"\\nThe Exact Match and F1 Score are: \\n\",end=\"\")\n",
        "                compute_f1(prediction,answer)\n",
        "                #print(f\"F1: {f1}\")\n",
        "                #print(\"\\n----------------------------\",end=\"\")\n",
        "\n",
        "                print(\"\\nDo you want to test another question?\\n\\n\"+name+\":\")\n",
        "        else:\n",
        "            flag=False\n",
        "            print(\"\\nMediBot: Bye\"+name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vnk3aup8d7Tx"
      },
      "source": [
        "4.4.4 Run the chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxAaclyHd7Ty"
      },
      "outputs": [],
      "source": [
        "def runchatbot():\n",
        "    flag=True\n",
        "    print(\"\\nMediBot: Hello \"+name+\"! Go ahead, you can ask me anything!\"+\"\\n\\n\"+name+\":\")\n",
        "    while(flag==True):\n",
        "  #  user_choice='no'\n",
        "        user_response = input()\n",
        "        user_response=user_response.lower()\n",
        "        if(user_response!='bye'):\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"\\nMediBot: \"+greeting(user_response)+\"\\n\\n\"+name+\":\")   \n",
        "            else:\n",
        "                answer=response(user_response)\n",
        "                print(\"\\nMediBot: \",end=\"\")\n",
        "                print(answer)\n",
        "                print(\"\\nDo you want to ask anything else?\\n\\n\"+name+\":\")\n",
        "        else:\n",
        "            flag=False\n",
        "            print(\"\\nMediBot: Bye \"+name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqjZocYWd7Tz"
      },
      "source": [
        "4.4.5 Choose to evaluate or run the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pKpnpx09d7Tz",
        "outputId": "44fa8495-b504-4e31-92a1-680960bde902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MediBot: Hi I am MediBot. I am an expert on medical staff. What's your name?\n"
          ]
        }
      ],
      "source": [
        "print(\"MediBot: Hi I am MediBot. I am an expert on medical staff. What's your name?\")\n",
        "name=input()\n",
        "print(\"\\nMediBot: Hello \"+name+\"! Do you want to evaluate or run the chatbot?\"+\"\\n\\n\"+name+\":\")\n",
        "eval_run = input()\n",
        "if (eval_run!='run'):\n",
        "    evaluation()\n",
        "else:\n",
        "    runchatbot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLH3FR70d7T0"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFaQw7SSd7T1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtHM0-6-d7T1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSCwfUGUd7T1"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Medibot.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}